---
layout: single
title:  "[PyTorch for Deep Learning] 01 PyTorch Workflow - 3. Train Model"
categories: [Deep Learning]
tag: [pytorch, coding, deeplearning]
use_math: true
---

이 글은 PyTorch workflow 공부에 관한 기록입니다.


# 3. Train model

The whole idea of training is for a model to move from some *unknown* parameters (these may be random) to some *known* parameters.

Or in other words from a poor representation of the data to a better representation of the data.

One way to measure how poor or how wrong your models predictions are is to use a loss function.

* Note: Loss function may also be called cost function or criterion in different areas. For our case, we're going to refer to it as a loss function.

Things we need to train:

* **Loss function:** A function to measure how wrong your model's predictions are to the ideal outputs, lower is better.

--> 학습된 모델이 얼마나 데이터를 잘 표현하고 있는지를 나타내는 함수

* **Optimizer:** Takes into account the loss of a model and adjusts the model's parameters (e.g. weight & bias in our case) to improve the loss function - [Optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim){: target="_blank"}
  * Inside the optimizer you'll often have to set two parameters:
    * `params` - the model parameters you'd like to optimize, for example `params=model_0.parameters()`
    * `lr` (learning rate) - the learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step (a small `lr` results in small changes, a large `lr` results in large changes)

--> 계산된 Loss를 바탕으로 모델의 파라미터를 수정함. `params`, `lr` 파라미터 설정을 잘 해줄 것.

And specifically for PyTorch, we need:
* A training loop
* A testing loop


```python
list(model_0.parameters())
```




    [Parameter containing:
     tensor([0.3367], requires_grad=True), Parameter containing:
     tensor([0.1288], requires_grad=True)]




```python
# Check out our model's parameters (a parameter is a value that the model sets itself)
model_0.state_dict()
```




    OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])




```python
# Setup a loss function
loss_fn = nn.L1Loss()

# Setup an optimizer (stochastic gradient descent)
optimizer = torch.optim.SGD(params=model_0.parameters(), # we want to optimize the parameters present in our model
                            lr=0.01) # lr = learning rate = possibly the most important hyperparameter you can set
```

> **Q:** Which loss function and optimizer should I use?
>
> **A:** This will be problem specific. But with experience, you'll get an idea of what works and what doesn't with your particular problem set.
>
> For example, for a regression problem (like ours), a loss function of `nn.L1Loss()` and an optimizer like `torch.optim.SGD()` will suffice.
>
> But for a classification problem like classifying whether a photo is of a dog or a cat, you'll likely want to use a loss function of `nn.BCELoss()` (binary cross entropy loss). 

--> 모델의 목적에 따라 Loss function을 적절히 설정해야할 것

## Building a training loop (and a testing loop) in PyTorch

A couple of things we need in a training loop:
0. Loop through the data and do...
1. Forward pass (this involves data moving through our model's `forward()` functions) to make predictions on data - also called forward propagation 
2. Calculate the loss (compare forward pass predictions to ground truth labels)
3. Optimizer zero grad
4. Loss backward - move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss ([Backpropagation](https://www.youtube.com/watch?v=tIeHLnjs5U8){: target="_blank"})
5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (**gradient descent** - https://youtu.be/IHZwWFHWa-w)

--> 이해를 하면 가장 좋고, 그냥 단순히 암기해도 절대 손해보지 않을 듯한 공식같은 느낌. 코드 따라 작성하면서 충분히 외울 수 있다.

```python
torch.manual_seed(42)

# An epoch is one loop through the data... (this is a hyperparameter because we've set it ourselves)
epochs = 200

# Track different values
epoch_count = [] 
loss_values = []
test_loss_values = [] 

### Training
# 0. Loop through the data
for epoch in range(epochs): 
  # Set the model to training mode
  model_0.train() # train mode in PyTorch sets all parameters that require gradients to require gradients 

  # 1. Forward pass
  y_pred = model_0(X_train)

  # 2. Calculate the loss
  loss = loss_fn(y_pred, y_train)

  # 3. Optimizer zero grad
  optimizer.zero_grad() 

  # 4. Perform backpropagation on the loss with respect to the parameters of the model (calculate gradients of each parameter)
  loss.backward()

  # 5. Step the optimizer (perform gradient descent)
  optimizer.step() # by default how the optimizer changes will acculumate through the loop so... we have to zero them above in step 3 for the next iteration of the loop

  ### Testing
  model_0.eval() # turns off different settings in the model not needed for evaluation/testing (dropout/batch norm layers)
  with torch.inference_mode(): # turns off gradient tracking & a couple more things behind the scenes - https://twitter.com/PyTorch/status/1437838231505096708?s=20&t=aftDZicoiUGiklEP179x7A
  # with torch.no_grad(): # you may also see torch.no_grad() in older PyTorch code
    # 1. Do the forward pass 
    test_pred = model_0(X_test)

    # 2. Calculate the loss
    test_loss = loss_fn(test_pred, y_test)

  # Print out what's happenin'
  if epoch % 10 == 0:
    epoch_count.append(epoch)
    loss_values.append(loss)
    test_loss_values.append(test_loss)
    print(f"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}")
    # Print out model state_dict()
    print(model_0.state_dict())
```

    Epoch: 0 | Loss: 0.31288138031959534 | Test loss: 0.48106518387794495
    OrderedDict([('weights', tensor([0.3406])), ('bias', tensor([0.1388]))])
    Epoch: 10 | Loss: 0.1976713240146637 | Test loss: 0.3463551998138428
    OrderedDict([('weights', tensor([0.3796])), ('bias', tensor([0.2388]))])
    Epoch: 20 | Loss: 0.08908725529909134 | Test loss: 0.21729660034179688
    OrderedDict([('weights', tensor([0.4184])), ('bias', tensor([0.3333]))])
    Epoch: 30 | Loss: 0.053148526698350906 | Test loss: 0.14464017748832703
    OrderedDict([('weights', tensor([0.4512])), ('bias', tensor([0.3768]))])
    Epoch: 40 | Loss: 0.04543796554207802 | Test loss: 0.11360953003168106
    OrderedDict([('weights', tensor([0.4748])), ('bias', tensor([0.3868]))])
    Epoch: 50 | Loss: 0.04167863354086876 | Test loss: 0.09919948130846024
    OrderedDict([('weights', tensor([0.4938])), ('bias', tensor([0.3843]))])
    Epoch: 60 | Loss: 0.03818932920694351 | Test loss: 0.08886633068323135
    OrderedDict([('weights', tensor([0.5116])), ('bias', tensor([0.3788]))])
    Epoch: 70 | Loss: 0.03476089984178543 | Test loss: 0.0805937647819519
    OrderedDict([('weights', tensor([0.5288])), ('bias', tensor([0.3718]))])
    Epoch: 80 | Loss: 0.03132382780313492 | Test loss: 0.07232122868299484
    OrderedDict([('weights', tensor([0.5459])), ('bias', tensor([0.3648]))])
    Epoch: 90 | Loss: 0.02788739837706089 | Test loss: 0.06473556160926819
    OrderedDict([('weights', tensor([0.5629])), ('bias', tensor([0.3573]))])
    Epoch: 100 | Loss: 0.024458957836031914 | Test loss: 0.05646304413676262
    OrderedDict([('weights', tensor([0.5800])), ('bias', tensor([0.3503]))])
    Epoch: 110 | Loss: 0.021020207554101944 | Test loss: 0.04819049686193466
    OrderedDict([('weights', tensor([0.5972])), ('bias', tensor([0.3433]))])
    Epoch: 120 | Loss: 0.01758546568453312 | Test loss: 0.04060482233762741
    OrderedDict([('weights', tensor([0.6141])), ('bias', tensor([0.3358]))])
    Epoch: 130 | Loss: 0.014155393466353416 | Test loss: 0.03233227878808975
    OrderedDict([('weights', tensor([0.6313])), ('bias', tensor([0.3288]))])
    Epoch: 140 | Loss: 0.010716589167714119 | Test loss: 0.024059748277068138
    OrderedDict([('weights', tensor([0.6485])), ('bias', tensor([0.3218]))])
    Epoch: 150 | Loss: 0.0072835334576666355 | Test loss: 0.016474086791276932
    OrderedDict([('weights', tensor([0.6654])), ('bias', tensor([0.3143]))])
    Epoch: 160 | Loss: 0.0038517764769494534 | Test loss: 0.008201557211577892
    OrderedDict([('weights', tensor([0.6826])), ('bias', tensor([0.3073]))])
    Epoch: 170 | Loss: 0.008932482451200485 | Test loss: 0.005023092031478882
    OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])
    Epoch: 180 | Loss: 0.008932482451200485 | Test loss: 0.005023092031478882
    OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])
    Epoch: 190 | Loss: 0.008932482451200485 | Test loss: 0.005023092031478882
    OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])

--> **자꾸 model.train(), model.eval()을 까먹는다. 신경써야한다.**


```python
import numpy as np
# Plot the loss curves
plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label="Train loss")
plt.plot(epoch_count, test_loss_values, label="Test loss")
plt.title("Training and test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend();
```


    
![01_pytorch_workflow_video_32_0](https://github.com/yesnote/yesnote.github.io/assets/173476188/0c9f5cf0-2127-46f2-858f-a4c23e0850f0)
    



```python
with torch.inference_mode():
  y_preds_new = model_0(X_test)
```


```python
model_0.state_dict()
```




    OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])




```python
weight, bias
```




    (0.7, 0.3)




```python
plot_predictions(predictions=y_preds);
```


    
![01_pytorch_workflow_video_36_0](https://github.com/yesnote/yesnote.github.io/assets/173476188/86735974-19bd-4fec-b8b8-3a28167190e8)
    



```python
plot_predictions(predictions=y_preds_new);
```


    
![01_pytorch_workflow_video_37_0](https://github.com/yesnote/yesnote.github.io/assets/173476188/63325eb0-9102-4c05-9d4f-e6b2829be1a0)

# 참고자료

[Ground truth notebook](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb){: target="_blank"}

[Book version of notebook](https://www.learnpytorch.io/01_pytorch_workflow/){: target="_blank"}

[Ask a question](https://github.com/mrdbourke/pytorch-deep-learning/discussions){: target="_blank"}