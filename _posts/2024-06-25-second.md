---
layout: single
title:  "[PyTorch for Deep Learning] PyTorch 기초"
categories: Pytorch
tag: [pytorch, coding, deeplearning]
use_math: true
---

이 글은 PyTorch 기초 공부에 관한 기록입니다.


## 텐서 (Tensor)



### Creating Tensor


```python
# scalar
scalar = torch.tensor(7)
scalar.ndim # 0
scalar.item() # 7

# Vector
vector = torch.tensor([7, 7])
vector.ndim # 1
vector.shape # torch.Size([2])

# MATRIX
MATRIX = torch.tensor([[7, 8],
                       [9, 10]])
MATRIX.ndim # 2
MATRIX[0] # tensor([7, 8])
MATRIX[1] # tensor([ 9, 10])
MATRIX.shape # torch.Size([2, 2])

# TENSOR
TENSOR = torch.tensor([[[1, 2, 3],
                        [3, 6, 9],
                        [2, 4, 5]]])
TENSOR.ndim # 3
TENSOR.shape # torch.Size([1, 3, 3])

TENSOR = torch.tensor([[[1, 2, 3],
                        [3, 6, 9],
                        [2, 4, 5]],
                        [[1, 2, 3],
                        [3, 6, 9],
                        [2, 4, 5]]])
TENSOR.ndim # 3
TENSOR.shape # torch.Size([2, 3, 3])
```



### Random Tensor

많은 신경망의 학습은 무작위한 수로 이루어진 텐서들에서 시작됩니다.

데이터를 잘 표현할 수 있는 방식으로 무작위한 수들을 조정함으로써 학습하는 방식이므로 Random Tensor는 학습에 중요한 역할을 합니다.

`Start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers`


```python
# Create a random tensor of size (3, 4)
random_tensor = torch.rand(3, 4)

# Create a random tensor with similar shape to an image tensor
random_image_size_tensor = torch.rand(size=(224, 224, 3)) # height, width, colour channels (R, G, B)
random_image_size_tensor.shape, random_image_size_tensor.ndim # (torch.Size([224, 224, 3]), 3)
```



### Zeros & Ones


```python
# Create a tensor of all zeros
zeros = torch.zeros(size=(3, 4))
zeros # tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]])

zeros * random_tensor # tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]])

# Create a tensor of all ones
ones = torch.ones(size=(3, 4))
ones # tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]])
ones.dtype # torch.float32
random_tensor.dtype # torch.float32
```



### Range of Tensor & Tensors-like


```python
# Use torch.range() and get deprecated message, use torch.arange()
one_to_ten = torch.arange(start=1, end=11, step=1)
one_to_ten # tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])

# Creating tensors like
ten_zeros = torch.zeros_like(input=one_to_ten)
ten_zeros # tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
```



### Tensor Datatype

**PyTorch & Deep Learning 주요 오류 3가지**
* Tensors not right datatype
* Tensors not right shape
* Tensors not on the right device


```python
# Float 32 tensor
float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                               dtype=None, # What datatype is the tensor (e.g.float32 or float16)
                               device=None, # What device is your tensor on
                               requires_grad=False) #Wheter or not to track gradients with this tensors operations
float_32_tensor # tensor([3., 6., 9.])
float_32_tensor.dtype # torch.float32

# Float 16 tensor
float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor # tensor([3., 6., 9.], dtype=torch.float16)
float_16_tensor * float_32_tensor # tensor([ 9., 36., 81.])

# Int 32 tensor
int_32_tensor = torch.tensor([3, 6, 9], dtype=torch.long)
int_32_tensor # tensor([3, 6, 9])
float_32_tensor * int_32_tensor # tensor([ 9., 36., 81.])
```



### Tensor Attribute

* Tensors not right datatype -> `tensor.dtype`
* Tensors not right shape -> `tensor.shape`
* Tensors not on the right device -> `tensor.device`


```python
# Create a tensor
some_tensor = torch.rand(3, 4)
some_tensor.size(), some_tensor.shape # (torch.Size([3, 4]), torch.Size([3, 4]))

# Find out details about some tensor
print(f"Datatype of tensor: {some_tensor.dtype}") # Datatype of tensor: torch.float32
print(f"Shape of tensor: {some_tensor.shape}") # Shape of tensor: torch.Size([3, 4])
print(f"Device tensor is on: {some_tensor.device}") # Device tensor is on: cpu
```



### Manipulating Tensor

**Tensor operations**
* Addition
* Substraction
* Multiplication (element-wise)
* Division
* Matrix multiplication


```python
# Create a tensor and add 10 to it
tensor = torch.tensor([1, 2, 3])
tensor + 10 # tensor([11, 12, 13])

# Multiply tensor by 10
tensor * 10 # tensor([10, 20, 30])

# Substract 10
tensor - 10 # tensor([-9, -8, -7])

# Try out PyTorch in-built functions
torch.mul(tensor, 10) # tensor([10, 20, 30])
torch.add(tensor, 10) # tensor([11, 12, 13])
```



### Matrix Multiplication

Rules for matrix multiplication:

1. The **inner dimensions** must match:
* `(3, 2) @ (3, 2)` won't work
* `(2, 3) @ (3, 2)` will work
* `(3, 2) @ (2, 3)` will work
2. The resulting matrix has the shape of the **outer dimensions**:
* `(2, 3) @ (3, 2)` -> `(2, 2)`
* `(3, 2) @ (2, 3)` -> `(3, 3)`


```python
torch.matmul(torch.rand(10, 10), torch.rand(10, 10)).shape # torch.Size([10, 10])

# Element wise multiplication
tensor * tensor # tensor([1, 4, 9])

# Matrix multiplication
torch.matmul(tensor, tensor) # tensor(14)

# Matrix multiplication by hand
1*1 + 2*2 + 3*3 # 14

%%time
value = 0
for i in range(len(tensor)):
  value += tensor[i] * tensor[i]
print(value)
"""
tensor(14)
CPU times: user 974 µs, sys: 984 µs, total: 1.96 ms
Wall time: 2.1 ms
"""

%%time
torch.matmul(tensor, tensor)
"""
CPU times: user 478 µs, sys: 0 ns, total: 478 µs
Wall time: 417 µs
tensor(14)
"""
```



### Shape Error

Shape error -> **Transpose**


```python
# Shapes for matrix multiplication
tensor_A = torch.tensor([[1, 2],
                         [3, 4],
                         [5, 6]])
tensor_B = torch.tensor([[7, 10],
                         [8, 11],
                         [9, 12]])

tensor_B.T, tensor_B # (tensor([[ 7,  8,  9], [10, 11, 12]]),
                     # (tensor([[ 7, 10], [ 8, 11], [ 9, 12]]))

tensor_B.T.shape, tensor_B.shape # (torch.Size([2, 3]), torch.Size([3, 2]))

# The matrix multiplication operation works when tensor_B is transposed
print(f"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}") # Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])
print(f"New shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.T.shape}") # New shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([2, 3])
print(f"Multiplying: {tensor_A.shape} @ {tensor_B.T.shape} <- inner dimensions must match") # Multiplying: torch.Size([3, 2]) @ torch.Size([2, 3]) <- inner dimensions must match
print("Output: \n")
output = torch.matmul(tensor_A, tensor_B.T) # tensor([[ 27,  30,  33],
                                                      [ 61,  68,  75],
                                                      [ 95, 106, 117]])
print(output)
print(f"\nOutput shape: {output.shape}") # Output shape: torch.Size([3, 3])
```



### Tensor Aggregation


```python
# Create a tensor
x = torch.arange(1, 100, 10)
x, x.dtype # (tensor([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91]), torch.int64)

# Find the min
torch.min(x), x.min() # (tensor(1), tensor(1))

# Find the max
torch.max(x), x.max() # (tensor(91), tensor(91))

# Find the mean - note: the torch.mean() function requires a tensor of float32 datatype to work
torch.mean(x.type(torch.float32)), x.type(torch.float32).mean() # (tensor(46.), tensor(46.))

# Find the sum
torch.sum(x), x.sum() # (tensor(460), tensor(460))
```



### Min, Max 값의 위치


```python
x # tensor([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])

# Find the position in tensor that has the minimum value with argmin() -> returns index position of target tensor where the minimum value occurs
x.argmin() # tensor(0)
x[0] # tensor(1)

# Find the position in tensor that has the maximum value with argmax()
x.argmax() # tensor(9)
x[9] # tensor(91)
```



### Reshaping, Stacking, Squeezing and Unsqueezing Tensor

* Reshaping
  * 텐서를 임의의 shape으로 변형합니다.
* View
  * 인풋 텐서를 임의의 shape으로 본 값을 반환합니다.
  * 동시에 원본 텐서와 같은 메모리를 공유합니다.
* Stacking
  * 다수의 텐서를 수직으로 (vstack) 혹은 수평으로 (hstack) 조합합니다.
* Squeeze
  * 모든 `1` dimension을 텐서로부터 제거합니다.
* Unsqueeze
  * 텐서에 `1` dimension을 추가합니다.
* Permute
  * 임의로 인풋 텐서의 dimension을 바꾼 값을 반환합니다.


```python
# Let's create a tensor
import torch
x = torch.arange(1., 10.)
x, x.shape # (tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), torch.Size([9]))

# Add an extra dimension
x_reshaped = x.reshape(1, 9)
x_reshaped, x_reshaped.shape # (tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))

# Change the view
z = x.view(1, 9) # (tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))
z, z.shape

# Changing z changes x (because a view of a tensor sgares the same memory as the original input)
z[:, 0] = 5
z, x # (tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]]),
     #  (tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.]))

# Stack tensors on top of each other
x_stacked = torch.stack([x, x, x, x], dim=0)
x_stacked
""" tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.],
[5., 2., 3., 4., 5., 6., 7., 8., 9.],
[5., 2., 3., 4., 5., 6., 7., 8., 9.],
[5., 2., 3., 4., 5., 6., 7., 8., 9.]]) """

# torch.squeeze() - removes all single dimensions from a target tensor
print(f"Previous tensor: {x_reshaped}") # Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])
print(f"Previous shape: {x_reshaped.shape}") # Previous shape: torch.Size([1, 9])

# Remove extra dimensions from x_reshaped
x_squeezed = x_reshaped.squeeze()
print(f"\nNew tensor: {x_squeezed}") # New tensor: tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])
print(f"New shape: {x_squeezed.shape}") # New shape: torch.Size([9])

# torch.unsqueeze() - adds a single dimension to a target tensor at a specific dim (dimension)
print(f"Previous tensor: {x_squeezed}") # Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])
print(f"Previous shape: {x_squeezed.shape}") # Previous shape: torch.Size([9])

# Add an extra dimension with unsqueeze
x_unsqueezed = x_squeezed.unsqueeze(dim=0)
print(f"\nNew tensor: {x_unsqueezed}") # New tensor: tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])
print(f"New shape: {x_unsqueezed.shape}") # New shape: torch.Size([1, 9])

# torch.permute - rearranges the dimensions of a target tensor in a specified order
x_original = torch.rand(size=(224, 224, 3)) # [height, width, colour_channels]

# Permute the original tensor to rearrange the axis (or dim) order
x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0

print(f"Previous shape: {x_original.shape}") # Previous shape: torch.Size([224, 224, 3])
print(f"New shape: {x_permuted.shape}") # [colour_channels, height, width] # New shape: torch.Size([3, 224, 224])
print(f"New shape: {torch.permute(x_original, (2, 0, 1)).shape}") # New shape: torch.Size([3, 224, 224])

x_original[0, 0, 0] = 100
x_original[0, 0, 0], x_permuted[0, 0, 0] # (tensor(100.), tensor(100.))
```



### Indexing


```python
# Create a tensor
import torch
x = torch.arange(1, 10).reshape(1, 3, 3)
x, x.shape # (tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]), torch.Size([1, 3, 3]))

# Let's index on our new tensor
x[0] # tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# Let's index on the middle bracket (dim=1)
x[0, 0] # tensor([1, 2, 3])

# Let's index on the most inner bracket (last dimension)
x[0, 2, 2] # tensor(9)

# You can also use ":" to select "all" of a target dimension
x[:, 0], x[:, 0].shape # (tensor([[1, 2, 3]]), torch.Size([1, 3]))

# Get all values of 0th and 1st dimension but only index 1 of 2nd dimension
x[:, :, 1], x[:, :, 1].shape # (tensor([[2, 5, 8]]), torch.Size([1, 3]))

x[0, :, 1], x[0, :, 1].shape # (tensor([2, 5, 8]), torch.Size([3]))

# Get all values of the 0 dimension but only the 1 index value of 1st and 2nd dimension
x[:, 1, 1], x[:, 1, 1].shape # (tensor([5]), torch.Size([1]))

# Get index 0 of 0th and 1st dimension and all values of 2nd dimension
x[0, 0, :], x[0, 0, :].shape # (tensor([1, 2, 3]), torch.Size([3]))

# Index on x to return 9
print(x[0, 2, 2]) # tensor(9)

# Index on x to return 3, 6, 9
print(x[:, :, 2]) # tensor([[3, 6, 9]])
```



### PyTorch tensors & NumPy

NumPy는 PyTorch와 기능적으로 상호작용할 수 있습니다.

* NumPy의 데이터를 PyTorch 텐서로 옮길 때 -> `torch.from_numpy(ndarray)`
* Pytorch 텐서를 NumPy 데이터로 옮길 때 -> `torch.Tensor.numpy()`


```python
# Numpy array to tensor
import torch
import numpy as np

array = np.arange(1.0, 8.0)
tensor = torch.from_numpy(array).type(torch.float32) # warning: when converting from numpy -> pytorch, pytorch reflects numpy's default datatype of float64 unless specified otherwise
array, tensor # (array([1., 2., 3., 4., 5., 6., 7.]), tensor([1., 2., 3., 4., 5., 6., 7.], dtype=float64))

# Change the value of array, what will this do to `tensor`?
array = array + 1
array, tensor # (array([2., 3., 4., 5., 6., 7., 8.]), tensor([1., 2., 3., 4., 5., 6., 7.]))

# Tensor to NumPy array
tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor # (tensor([1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))

# Change the tensor, what happens to `numpy_tensor`?
tensor = tensor + 1
tensor, numpy_tensor # (tensor([2., 2., 2., 2., 2., 2., 2.]), array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))
```



## Reproducibility

신경망과 PyTorch의 무작위성을 줄이기 위해 **random seed**를 사용합니다.

Random seed를 이용해 신경망과 PyTorch의 무작위성을 "입맛에 맞게" 맞출 수 있습니다.


```python
import torch

# Create two random tensors
random_tensor_A = torch.rand(3, 4)
random_tensor_B = torch.rand(3, 4)

print(random_tensor_A == random_tensor_B) # tensor([[False ... False]])

# Let's make some random but reproducible tensors
import torch

# Set the random seed
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
random_tensor_C = torch.rand(3, 4)

torch.manual_seed(RANDOM_SEED)
random_tensor_D = torch.rand(3, 4)

print(random_tensor_C == random_tensor_D) # tensor([[True ... True]])
```



## Running Tensor & PyTorch object on the GPUs

CUDA + NVIDIA hardward + PyTorch working -> GPU의 빠른 연산 속도



### 1. Getting a GPU

1. Easiest - Use Google Colab for a free GPU (options to upgrade as well)
2. Use your own GPU - takes a little bit of setup and requires the investment of purchasing a GPU, there's a lots of options... see this post for what option to get: https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/
3. Use cloud computing - GCP, AWS, Azure, these services allow you to rent computers on the cloud and access them



### 2. Check for GPU access with PyTorch


```python
# Check for GPU access with PyTorch
import torch
torch.cuda.is_available() # True

# Setup device agnostic code
device = "cuda" if torch.cuda.is_available else "cpu"
device # 'cuda'

# Count number of devices
torch.cuda.device_count() # 1
```



### 3. Putting tensors (and models) on the GPU

GPU의 빠른 연산 속도를 활용하기 위해 텐서 혹은 모델을 GPU에 올립니다.

```python
# Create a tensor (default on the CPU)
tensor = torch.tensor([1, 2, 3], device = "cpu")

# Tensor not on GPU
print(tensor, tensor.device) # tensor([1, 2, 3]) cpu

# Move tensor to GPU (if available)
tensor_on_gpu = tensor.to(device)
tensor_on_gpu # tensor([1, 2, 3], device='cuda:0')
```



### 4. Moving tensors back to the CPU


```python
# If tensor is on GPU, can't transform it to NumPy
tensor_on_gpu.numpy() # Typeerror 발생

# To fix the GPU tensor with NumPy issue, we can first set it to the CPU
tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu # array([1, 2, 3])
tensor_on_gpu # tensor([1, 2, 3], device='cuda:0')
```



## Exercises & Extra-curriculum

[Exercises for this notebook](https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises){: target="_blank"}



## 참고자료

[PyTorch tensors are created using 'torch.tensor()'](https://pytorch.org/docs/stable/tensors.html){: target="_blank"}

[Torch random tensors](https://pytorch.org/docs/stable/generated/torch.rand.html){: target="_blank"}

[Precision in computing](https://en.wikipedia.org/wiki/Single-precision_floating-point_format){: target="_blank"}

[Extra resources for reproducibility 1](https://pytorch.org/docs/stable/notes/randomness.html){: target="_blank"}

[Extra resources for reproducibility 2](https://en.wikipedia.org/wiki/Random_seed){: target="_blank"}

[Setup documentations](https://pytorch.org/get-started/locally/){: target="_blank"}

[Setup device agnostic code](https://pytorch.org/docs/stable/notes/cuda.html#best-practices){: target="_blank"}
