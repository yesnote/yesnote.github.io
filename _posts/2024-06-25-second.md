---
layout: single
title:  "[PyTorch for Deep Learning] PyTorch Fundamentals"
categories: Pytorch
tag: [pytorch, coding, deeplearning]
use_math: true
---

이 글은 PyTorch Fundamentals에 관한 기록입니다.


## Introductions to Tensors

### Creating Tensors

[PyTorch tensors are created using 'torch.tensor()'](https://pytorch.org/docs/stable/tensors.html){: target="_blank"}


```python
# scalar
scalar = torch.tensor(7)
scalar.ndim # 0
scalar.item() # 7

# Vector
vector = torch.tensor([7, 7])
vector.ndim # 1
vector.shape # torch.Size([2])

# MATRIX
MATRIX = torch.tensor([[7, 8],
                       [9, 10]])
MATRIX.ndim # 2
MATRIX[0] # tensor([7, 8])
MATRIX[1] # tensor([ 9, 10])
MATRIX.shape # torch.Size([2, 2])

# TENSOR
TENSOR = torch.tensor([[[1, 2, 3],
                        [3, 6, 9],
                        [2, 4, 5]]])
TENSOR.ndim # 3
TENSOR.shape # torch.Size([1, 3, 3])

TENSOR = torch.tensor([[[1, 2, 3],
                        [3, 6, 9],
                        [2, 4, 5]],
                        [[1, 2, 3],
                        [3, 6, 9],
                        [2, 4, 5]]])
TENSOR.ndim # 3
TENSOR.shape # torch.Size([2, 3, 3])
```


### Random tensors

많은 신경망의 학습은 무작위한 수로 이루어진 텐서들에서 시작됩니다.

데이터를 잘 표현할 수 있는 방식으로 무작위한 수들을 조정함으로써 학습하는 방식이므로 Random Tensor는 학습에 중요한 역할을 합니다.

`Start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers`

[Torch random tensors](https://pytorch.org/docs/stable/generated/torch.rand.html){: target="_blank"}


```python
# Create a random tensor of size (3, 4)
random_tensor = torch.rand(3, 4)

# Create a random tensor with similar shape to an image tensor
random_image_size_tensor = torch.rand(size=(224, 224, 3)) # height, width, colour channels (R, G, B)
random_image_size_tensor.shape, random_image_size_tensor.ndim # (torch.Size([224, 224, 3]), 3)
```


### Zeros and Ones


```python
# Create a tensor of all zeros
zeros = torch.zeros(size=(3, 4))
zeros # tensor([[0., 0., 0., 0.],
                [0., 0., 0., 0.],
                [0., 0., 0., 0.]])

zeros * random_tensor # tensor([[0., 0., 0., 0.],
                                [0., 0., 0., 0.],
                                [0., 0., 0., 0.]])

# Create a tensor of all ones
ones = torch.ones(size=(3, 4))
ones # tensor([[1., 1., 1., 1.],
              [1., 1., 1., 1.],
              [1., 1., 1., 1.]])
ones.dtype # torch.float32
random_tensor.dtype # torch.float32
```


### Creating a range of tensors and tensors-like


```python
# Use torch.range() and get deprecated message, use torch.arange()
one_to_ten = torch.arange(start=1, end=11, step=1)
one_to_ten # tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])

# Creating tensors like
ten_zeros = torch.zeros_like(input=one_to_ten)
ten_zeros # tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
```


### Tensor datatypes

**PyTorch & Deep Learning 주요 오류 3가지**
* Tensors not right datatype
* Tensors not right shape
* Tensors not on the right device

[Precision in computing](https://en.wikipedia.org/wiki/Single-precision_floating-point_format){: target="_blank"}


```python
# Float 32 tensor
float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                               dtype=None, # What datatype is the tensor (e.g.float32 or float16)
                               device=None, # What device is your tensor on
                               requires_grad=False) #Wheter or not to track gradients with this tensors operations
float_32_tensor # tensor([3., 6., 9.])
float_32_tensor.dtype # torch.float32

# Float 16 tensor
float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor # tensor([3., 6., 9.], dtype=torch.float16)
float_16_tensor * float_32_tensor # tensor([ 9., 36., 81.])

# Int 32 tensor
int_32_tensor = torch.tensor([3, 6, 9], dtype=torch.long)
int_32_tensor # tensor([3, 6, 9])
float_32_tensor * int_32_tensor # tensor([ 9., 36., 81.])
```


### Getting information from tensors (tensors attribute)

* Tensors not right datatype -> `tensor.dtype`
* Tensors not right shape -> `tensor.shape`
* Tensors not on the right device -> `tensor.device`


```python
# Create a tensor
some_tensor = torch.rand(3, 4)
some_tensor.size(), some_tensor.shape # (torch.Size([3, 4]), torch.Size([3, 4]))

# Find out details about some tensor
print(f"Datatype of tensor: {some_tensor.dtype}") # Datatype of tensor: torch.float32
print(f"Shape of tensor: {some_tensor.shape}") # Shape of tensor: torch.Size([3, 4])
print(f"Device tensor is on: {some_tensor.device}") # Device tensor is on: cpu
```


### Manipulating Tensors (tensor operations)

Tensor operations include:
* Addition
* Substraction
* Multiplication (element-wise)
* Division
* Matrix multiplication


```python
# Create a tensor and add 10 to it
tensor = torch.tensor([1, 2, 3])
tensor + 10 # tensor([11, 12, 13])

# Multiply tensor by 10
tensor * 10 # tensor([10, 20, 30])

# Substract 10
tensor - 10 # tensor([-9, -8, -7])

# Try out PyTorch in-built functions
torch.mul(tensor, 10) # tensor([10, 20, 30])
torch.add(tensor, 10) # tensor([11, 12, 13])
```


### Matrix multiplication

Rules for matrix multiplication:

1. The **inner dimensions** must match:
* `(3, 2) @ (3, 2)` won't work
* `(2, 3) @ (3, 2)` will work
* `(3, 2) @ (2, 3)` will work
2. The resulting matrix has the shape of the **outer dimensions**:
* `(2, 3) @ (3, 2)` -> `(2, 2)`
* `(3, 2) @ (2, 3)` -> `(3, 3)`


```python
torch.matmul(torch.rand(10, 10), torch.rand(10, 10)).shape # torch.Size([10, 10])

# Element wise multiplication
tensor * tensor # tensor([1, 4, 9])

# Matrix multiplication
torch.matmul(tensor, tensor) # tensor(14)

# Matrix multiplication by hand
1*1 + 2*2 + 3*3 # 14

%%time
value = 0
for i in range(len(tensor)):
  value += tensor[i] * tensor[i]
print(value)
# tensor(14)
# CPU times: user 974 µs, sys: 984 µs, total: 1.96 ms
# Wall time: 2.1 ms

%%time
torch.matmul(tensor, tensor)
# CPU times: user 478 µs, sys: 0 ns, total: 478 µs
# Wall time: 417 µs
# tensor(14)
```


### Shape Error

Shape error -> **Transpose**


```python
# Shapes for matrix multiplication
tensor_A = torch.tensor([[1, 2],
                         [3, 4],
                         [5, 6]])
tensor_B = torch.tensor([[7, 10],
                         [8, 11],
                         [9, 12]])

tensor_B.T, tensor_B # (tensor([[ 7,  8,  9],
                                 [10, 11, 12]]),
                        tensor([[ 7, 10],
                                [ 8, 11],
                                [ 9, 12]]))

tensor_B.T.shape, tensor_B.shape # (torch.Size([2, 3]), torch.Size([3, 2]))

# The matrix multiplication operation works when tensor_B is transposed
print(f"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}") # Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])
print(f"New shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.T.shape}") # New shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([2, 3])
print(f"Multiplying: {tensor_A.shape} @ {tensor_B.T.shape} <- inner dimensions must match") # Multiplying: torch.Size([3, 2]) @ torch.Size([2, 3]) <- inner dimensions must match
print("Output: \n")
output = torch.matmul(tensor_A, tensor_B.T) # tensor([[ 27,  30,  33],
                                                      [ 61,  68,  75],
                                                      [ 95, 106, 117]])
print(output)
print(f"\nOutput shape: {output.shape}") # Output shape: torch.Size([3, 3])
```


## Finding the min, max, mean, sum, etc (tensor aggregation)


```python
# Create a tensor
x = torch.arange(1, 100, 10)
x, x.dtype # (tensor([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91]), torch.int64)

# Find the min
torch.min(x), x.min() # (tensor(1), tensor(1))

# Find the max
torch.max(x), x.max() # (tensor(91), tensor(91))

# Find the mean - note: the torch.mean() function requires a tensor of float32 datatype to work
torch.mean(x.type(torch.float32)), x.type(torch.float32).mean() # (tensor(46.), tensor(46.))

# Find the sum
torch.sum(x), x.sum() # (tensor(460), tensor(460))
```


## Finding the positional min and max


```python
x # tensor([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])

# Find the position in tensor that has the minimum value with argmin() -> returns index position of target tensor where the minimum value occurs
x.argmin() # tensor(0)
x[0] # tensor(1)

# Find the position in tensor that has the maximum value with argmax()
x.argmax() # tensor(9)
x[9] # tensor(91)
```


## Reshaping, stacking, squeezing and unsqueezing tensors

* Reshaping
** 텐서를 정의된 shape으로 변형합니다.
* View - Return a view of an input tensor of certain shape but keep the same memory
as the original tensor 
* Stacking - combine multiple tensors on top of each other (vstack) or side by side (hstack)
* Squeeze - removes all `1` dimensions from a tensor
* Unsqueeze - add a `1` dimension to a target tensor
* Permute - Return a view of the input with dimensions permuted (swapped) in a certain way


```python
# Let's create a tensor
import torch
x = torch.arange(1., 10.)
x, x.shape (tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), torch.Size([9]))

# Add an extra dimension
x_reshaped = x.reshape(1, 9)
x_reshaped, x_reshaped.shape # (tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))

# Change the view
z = x.view(1, 9) # (tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))
z, z.shape

# Changing z changes x (because a view of a tensor sgares the same memory as the original input)
z[:, 0] = 5
z, x # (tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]]),
        tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.]))

# Stack tensors on top of each other
x_stacked = torch.stack([x, x, x, x], dim=0)
x_stacked # tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.],
                    [5., 2., 3., 4., 5., 6., 7., 8., 9.],
                    [5., 2., 3., 4., 5., 6., 7., 8., 9.],
                    [5., 2., 3., 4., 5., 6., 7., 8., 9.]])

# torch.squeeze() - removes all single dimensions from a target tensor
print(f"Previous tensor: {x_reshaped}") # Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])
print(f"Previous shape: {x_reshaped.shape}") # Previous shape: torch.Size([1, 9])

# Remove extra dimensions from x_reshaped
x_squeezed = x_reshaped.squeeze()
print(f"\nNew tensor: {x_squeezed}") # New tensor: tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])
print(f"New shape: {x_squeezed.shape}") # New shape: torch.Size([9])

# torch.unsqueeze() - adds a single dimension to a target tensor at a specific dim (dimension)
print(f"Previous tensor: {x_squeezed}") # Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])
print(f"Previous shape: {x_squeezed.shape}") # Previous shape: torch.Size([9])

# Add an extra dimension with unsqueeze
x_unsqueezed = x_squeezed.unsqueeze(dim=0)
print(f"\nNew tensor: {x_unsqueezed}") # New tensor: tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])
print(f"New shape: {x_unsqueezed.shape}") # New shape: torch.Size([1, 9])

# torch.permute - rearranges the dimensions of a target tensor in a specified order
x_original = torch.rand(size=(224, 224, 3)) # [height, width, colour_channels]

# Permute the original tensor to rearrange the axis (or dim) order
x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0

print(f"Previous shape: {x_original.shape}") # Previous shape: torch.Size([224, 224, 3])
print(f"New shape: {x_permuted.shape}") # [colour_channels, height, width] # New shape: torch.Size([3, 224, 224])
print(f"New shape: {torch.permute(x_original, (2, 0, 1)).shape}") # New shape: torch.Size([3, 224, 224])

x_original[0, 0, 0] = 100
x_original[0, 0, 0], x_permuted[0, 0, 0] # (tensor(100.), tensor(100.))
```


## Indexing (selecting data from tensors)

Indexing with PyTorch is similar to indexing with NumPy.


```python
# Create a tensor
import torch
x = torch.arange(1, 10).reshape(1, 3, 3)
x, x.shape
```




    (tensor([[[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9]]]),
     torch.Size([1, 3, 3]))




```python
# Let's index on our new tensor
x[0]
```




    tensor([[1, 2, 3],
            [4, 5, 6],
            [7, 8, 9]])




```python
# Let's index on the middle bracket (dim=1)
x[0, 0]
```




    tensor([1, 2, 3])




```python
# Let's index on the most inner bracket (last dimension)
x[0, 2, 2]
```




    tensor(9)




```python
# You can also use ":" to select "all" of a target dimension
x[:, 0], x[:, 0].shape
```




    (tensor([[1, 2, 3]]), torch.Size([1, 3]))




```python
# Get all values of 0th and 1st dimension but only index 1 of 2nd dimension
x[:, :, 1], x[:, :, 1].shape
```




    (tensor([[2, 5, 8]]), torch.Size([1, 3]))




```python
x[0, :, 1], x[0, :, 1].shape
```




    (tensor([2, 5, 8]), torch.Size([3]))




```python
# Get all values of the 0 dimension but only the 1 index value of 1st and 2nd dimension
x[:, 1, 1], x[:, 1, 1].shape
```




    (tensor([5]), torch.Size([1]))




```python
# Get index 0 of 0th and 1st dimension and all values of 2nd dimension
x[0, 0, :], x[0, 0, :].shape
```




    (tensor([1, 2, 3]), torch.Size([3]))




```python
# Index on x to return 9
print(x[0, 2, 2])

# Index on x to return 3, 6, 9
print(x[:, :, 2])
```

    tensor(9)
    tensor([[3, 6, 9]])


## PyTorch tensors & NumPy

NumPy is a popular scientific Python numerical computing library.

And because of this, PyTorch has functionally to interact with it.

* Data in NumPy, want in PyTorch tensor -> `torch.from_numpy(ndarray)`
* PyTorch tensor -> NumPy -> `torch.Tensor.numpy()`


```python
# Numpy array to tensor
import torch
import numpy as np

array = np.arange(1.0, 8.0)
tensor = torch.from_numpy(array).type(torch.float32) # warning: when converting from numpy -> pytorch, pytorch reflects numpy's default datatype of float64 unless specified otherwise
array, tensor
```




    (array([1., 2., 3., 4., 5., 6., 7.]), tensor([1., 2., 3., 4., 5., 6., 7.]))




```python
# Change the value of array, what will this do to `tensor`?
array = array + 1
array, tensor
```




    (array([2., 3., 4., 5., 6., 7., 8.]), tensor([1., 2., 3., 4., 5., 6., 7.]))




```python
# Tensor to NumPy array
tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor
```




    (tensor([1., 1., 1., 1., 1., 1., 1.]),
     array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))




```python
# Change the tensor, what happens to `numpy_tensor`?
tensor = tensor + 1
tensor, numpy_tensor
```




    (tensor([2., 2., 2., 2., 2., 2., 2.]),
     array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))



## Reproducibility (trying to take random out of random)

In short how a neural network learns:

`start with random numbers -> tensor operations -> update random numbers to try and make them better representations of the data -> again -> again -> again...`

To reduce the randomness in neural networks and PyTorch comes the concept of a **random seed**.

Essentially what the random seed does is "flavour" the randomness.


```python
import torch

# Create two random tensors
random_tensor_A = torch.rand(3, 4)
random_tensor_B = torch.rand(3, 4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)
```

    tensor([[0.8856, 0.3551, 0.7646, 0.4196],
            [0.0218, 0.0301, 0.7413, 0.2346],
            [0.8091, 0.7594, 0.8159, 0.8519]])
    tensor([[0.7222, 0.0376, 0.4321, 0.9269],
            [0.5496, 0.9051, 0.4669, 0.8847],
            [0.1647, 0.4106, 0.2032, 0.6142]])
    tensor([[False, False, False, False],
            [False, False, False, False],
            [False, False, False, False]])



```python
# Let's make some random but reproducible tensors
import torch

# Set the random seed
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
random_tensor_C = torch.rand(3, 4)

torch.manual_seed(RANDOM_SEED)
random_tensor_D = torch.rand(3, 4)

print(random_tensor_C)
print(random_tensor_D)
print(random_tensor_C == random_tensor_D)
```

    tensor([[0.8823, 0.9150, 0.3829, 0.9593],
            [0.3904, 0.6009, 0.2566, 0.7936],
            [0.9408, 0.1332, 0.9346, 0.5936]])
    tensor([[0.8823, 0.9150, 0.3829, 0.9593],
            [0.3904, 0.6009, 0.2566, 0.7936],
            [0.9408, 0.1332, 0.9346, 0.5936]])
    tensor([[True, True, True, True],
            [True, True, True, True],
            [True, True, True, True]])


Extra resources for reproducibility:
* [Extra resources for reproducibility 1](https://pytorch.org/docs/stable/notes/randomness.html){: target="_blank"}
* [Extra resources for reproducibility 2](https://en.wikipedia.org/wiki/Random_seed){: target="_blank"}

## Running tensors and PyTorch objects on the GPUs (and making faster computations)

GPUs = faster computation on numbers, thanks to CUDA + NVIDIA hardware + PyTorch working behind the scenes to make everything hunky dory (good).

### 1. Getting a GPU

1. Easiest - Use Google Colab for a free GPU (options to upgrade as well)
2. Use your own GPU - takes a little bit of setup and requires the investment of purchasing a GPU, there's a lots of options... see this post for what option to get: https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/
3. Use cloud computing - GCP, AWS, Azure, these services allow you to rent computers on the cloud and access them

For 2, 3 PyTorch + GPU drivers (CUDA) takes a little bit of setting up, to do this, refer to PyTorch setup documentations:
[Setup documentations](https://pytorch.org/get-started/locally/){: target="_blank"}


```python
!nvidia-smi
```

    Tue Jun 25 05:36:55 2024       
    +---------------------------------------------------------------------------------------+
    | NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
    |-----------------------------------------+----------------------+----------------------+
    | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
    |                                         |                      |               MIG M. |
    |=========================================+======================+======================|
    |   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |
    | N/A   42C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |
    |                                         |                      |                  N/A |
    +-----------------------------------------+----------------------+----------------------+
                                                                                             
    +---------------------------------------------------------------------------------------+
    | Processes:                                                                            |
    |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
    |        ID   ID                                                             Usage      |
    |=======================================================================================|
    |  No running processes found                                                           |
    +---------------------------------------------------------------------------------------+


### 2. Check for GPU access with PyTorch


```python
# Check for GPU access with PyTorch
import torch
torch.cuda.is_available()
```




    True



For PyTorch since it's capable of running compute on the GPU or CPU, it's best practice to setup device agnostic code:
[Setup device agnostic code](https://pytorch.org/docs/stable/notes/cuda.html#best-practices){: target="_blank"}

E.g. run on GPU if available, else default to CPU


```python
# Setup device agnostic code
device = "cuda" if torch.cuda.is_available else "cpu"
device
```




    'cuda'




```python
# Count number of devices
torch.cuda.device_count()
```




    1



### 3. Putting tensors (and models) on the GPU

The reason we want our tensors/models on the GPU is because using a GPU results in faster computations.


```python
# Create a tensor (default on the CPU)
tensor = torch.tensor([1, 2, 3], device = "cpu")

# Tensor not on GPU
print(tensor, tensor.device)
```

    tensor([1, 2, 3]) cpu



```python
# Move tensor to GPU (if available)
tensor_on_gpu = tensor.to(device)
tensor_on_gpu
```




    tensor([1, 2, 3], device='cuda:0')



### 4. Moving tensors back to the CPU


```python
# If tensor is on GPU, can't transform it to NumPy
tensor_on_gpu.numpy()
```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-101-b7da913938a5> in <cell line: 2>()
          1 # If tensor is on GPU, can't transform it to NumPy
    ----> 2 tensor_on_gpu.numpy()
    

    TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.



```python
# To fix the GPU tensor with NumPy issue, we can first set it to the CPU
```


```python
tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu
```




    array([1, 2, 3])




```python
tensor_on_gpu
```




    tensor([1, 2, 3], device='cuda:0')



## Exercises & Extra-curriculum

See exercises for this notebook here: https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises
[Exercises](https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises){: target="_blank"}
