---
layout: single
title:  "[PyTorch for Deep Learning] 02 Neural Network classification - 3. Train Model"
categories: [Machine Learning]
tag: [pytorch, coding]
use_math: true
---

이 글은 PyTorch를 이용한 Neural network classification 공부에 관한 기록입니다.


# Train model 

To train our model, we're going to need to build a training loop with the following steps:

1. Forward pass
2. Calculate the loss
3. Optimizer zero grad
4. Loss backward (backpropagation)
5. Optimizer step (gradient descent) 

## Going from raw logits -> prediction probabilities -> prediction labels

Our model outputs are going to be raw **logits**.

We can convert these **logits** into **prediction probabilities** by passing them to some kind of activation function (e.g. sigmoid for binary classification and softmax for multiclass classification).

Then we can convert our model's prediction probabilities to **prediction labels** by either rounding them or taking the `argmax()`.

--> logits: 모델의 출력 그자체

--> logits를 Activation function (sigmoid or softmax)를 통해 확률화

--> 가장 높은 확률에 해당하는 class를 출력함

```python
# View the first 5 outputs of the forward pass on the test data
model_0.eval() 
with torch.inference_mode():
  y_logits = model_0(X_test.to(device))[:5]
y_logits 
```




    tensor([[-0.1481],
            [-0.1465],
            [-0.0762],
            [-0.1688],
            [ 0.0445]], device='cuda:0')




```python
y_test[:5]
```




    tensor([1., 0., 1., 0., 1.])




```python
# Use the sigmoid activation function on our model logits to turn them into prediction probabilities
y_pred_probs = torch.sigmoid(y_logits)
y_pred_probs
```




    tensor([[0.4630],
            [0.4634],
            [0.4810],
            [0.4579],
            [0.5111]], device='cuda:0')



For our prediction probability values, we need to perform a range-style rounding on them:
* `y_pred_probs` >= 0.5, `y=1` (class 1)
* `y_pred_probs` < 0.5, `y=0` (class 0)


```python
# Find the predicted labels 
y_preds = torch.round(y_pred_probs)

# In full (logits -> pred probs -> pred labels)
y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))

# Check for equality
print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))

# Get rid of extra dimension
y_preds.squeeze()
```

    tensor([True, True, True, True, True], device='cuda:0')





    tensor([0., 0., 0., 0., 1.], device='cuda:0')




```python
y_test[:5]
```




    tensor([1., 0., 1., 0., 1.])



## Building a training and testing loop


```python
torch.manual_seed(42)
torch.cuda.manual_seed(42) 

# Set the number of epochs
epochs = 100

# Put data to target device 
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

# Build training and evaluation loop
for epoch in range(epochs):
  ### Training
  model_0.train()

  # 1. Forward pass
  y_logits = model_0(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels

  # 2. Calculate loss/accuracy
  # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss expects prediction probabilities as input
  #                y_train)
  loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss expects raw logits as input
                 y_train)
  acc = accuracy_fn(y_true=y_train, 
                    y_pred=y_pred)
  
  # 3. Optimizer zero grad
  optimizer.zero_grad()

  # 4. Loss backward (backpropagation)
  loss.backward()

  # 5. Optimizer step (gradient descent)
  optimizer.step() 

  ### Testing
  model_0.eval()
  with torch.inference_mode():
    # 1. Forward pass 
    test_logits = model_0(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    # 2. Calculate test loss/acc
    test_loss = loss_fn(test_logits,
                        y_test)
    test_acc = accuracy_fn(y_true=y_test,
                           y_pred=test_pred)
  
  # Print out what's happenin'
  if epoch % 10 == 0:
    print(f"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")
```

    Epoch: 0 | Loss: 0.69461, Acc: 47.88% | Test loss: 0.69301, Test acc: 48.50%
    Epoch: 10 | Loss: 0.69402, Acc: 48.75% | Test loss: 0.69296, Test acc: 48.00%
    Epoch: 20 | Loss: 0.69369, Acc: 49.50% | Test loss: 0.69308, Test acc: 46.50%
    Epoch: 30 | Loss: 0.69348, Acc: 50.62% | Test loss: 0.69325, Test acc: 45.50%
    Epoch: 40 | Loss: 0.69334, Acc: 50.12% | Test loss: 0.69341, Test acc: 48.50%
    Epoch: 50 | Loss: 0.69324, Acc: 49.25% | Test loss: 0.69357, Test acc: 51.00%
    Epoch: 60 | Loss: 0.69317, Acc: 49.50% | Test loss: 0.69372, Test acc: 50.50%
    Epoch: 70 | Loss: 0.69312, Acc: 50.38% | Test loss: 0.69385, Test acc: 49.00%
    Epoch: 80 | Loss: 0.69308, Acc: 50.12% | Test loss: 0.69396, Test acc: 49.50%
    Epoch: 90 | Loss: 0.69305, Acc: 50.50% | Test loss: 0.69406, Test acc: 48.00%

--> 원형 데이터를 선형 모델로 잘 표현할 수 없어서 결과가 좋지 않다.

# 참고자료

[Ground truth notebook](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/02_pytorch_classification.ipynb){: target="_blank"}

[Book version of notebook](https://www.learnpytorch.io/02_pytorch_classification/){: target="_blank"}

[Ask a question](https://github.com/mrdbourke/pytorch-deep-learning/discussions){: target="_blank"}