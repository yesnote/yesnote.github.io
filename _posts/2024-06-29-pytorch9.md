---
layout: single
title:  "[PyTorch for Deep Learning] 01 PyTorch Workflow - Exercises"
categories: [Machine Learning]
tag: [pytorch, coding]
use_math: true
---

이 글은 PyTorch workflow Exercises 문제 풀이 기록입니다.

분명 다 외웠다고 생각했는데 막상 문제 풀다보니 조금 헷갈리는 부분도 있었다. 간단한 Workflow 인 만큼 익숙해져야한다.

# Exercises

## Setting


```python
# Import necessary libraries
import torch
from torch import nn
import matplotlib.pyplot as plt
from pathlib import Path
import numpy as np
```


```python
# Setup device-agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
```


## 1. Create a straight line dataset using the linear regression formula (`weight * X + bias`).
  * Set `weight=0.3` and `bias=0.9` there should be at least 100 datapoints total.
  * Split the data into 80% training, 20% testing.
  * Plot the training and testing data so it becomes visual.

Your output of the below cell should look something like:
```
Number of X samples: 100
Number of y samples: 100
First 10 X & y samples:
X: tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,
        0.0900])
y: tensor([0.9000, 0.9030, 0.9060, 0.9090, 0.9120, 0.9150, 0.9180, 0.9210, 0.9240,
        0.9270])
```

Of course the numbers in `X` and `y` may be different but ideally they're created using the linear regression formula.


```python
# Create the data parameters
weight = 0.3
bias = 0.9

# Make X and y using linear regression feature
start = 0
end = 1
step = 0.01
X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias

print(f"Number of X samples: {len(X)}")
print(f"Number of y samples: {len(y)}")
print(f"First 10 X & y samples:\nX: {X[:10]}\ny: {y[:10]}")
```

    Number of X samples: 100
    Number of y samples: 100
    First 10 X & y samples:
    X: tensor([[0.0000],
            [0.0100],
            [0.0200],
            [0.0300],
            [0.0400],
            [0.0500],
            [0.0600],
            [0.0700],
            [0.0800],
            [0.0900]])
    y: tensor([[0.9000],
            [0.9030],
            [0.9060],
            [0.9090],
            [0.9120],
            [0.9150],
            [0.9180],
            [0.9210],
            [0.9240],
            [0.9270]])



```python
# Split the data into training and testing
train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]
```


```python
# Plot the training and testing data
def plot_predictions(train_data=X_train,
                     train_labels=y_train,
                     test_data=X_test,
                     test_labels=y_test,
                     predictions=None):
  plt.figure(figsize=(10, 7))
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data")
  plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data")
  if predictions is not None:
    plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")
  plt.legend(prop={"size": 14})

plot_predictions()
```


    
![01_pytorch_workflow_exercises_7_0](https://github.com/yesnote/yesnote.github.io/assets/173476188/376e03bb-addd-4a28-b784-102cbdaf0d52)


## 2. Build a PyTorch model by subclassing `nn.Module`.
  * Inside should be a randomly initialized `nn.Parameter()` with `requires_grad=True`, one for `weights` and one for `bias`.
  * Implement the `forward()` method to compute the linear regression function you used to create the dataset in 1.
  * Once you've constructed the model, make an instance of it and check its `state_dict()`.
  * **Note:** If you'd like to use `nn.Linear()` instead of `nn.Parameter()` you can.


```python
# Create PyTorch linear regression model by subclassing nn.Module
class LinearRegressionModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear_layer = nn.Linear(in_features=1,
                                  out_features=1)

  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.linear_layer(x)
```


```python
# Instantiate the model and put it to the target device
torch.manual_seed(42)
model_1 = LinearRegressionModel()
model_1.state_dict()
```




    OrderedDict([('linear_layer.weight', tensor([[0.7645]])),
                 ('linear_layer.bias', tensor([0.8300]))])




```python
next(model_1.parameters()).device
```




    device(type='cpu')




```python
model_1.to(device)
list(model_1.parameters())
```




    [Parameter containing:
     tensor([[0.7645]], device='cuda:0', requires_grad=True),
     Parameter containing:
     tensor([0.8300], device='cuda:0', requires_grad=True)]


## 3. Create a loss function and optimizer using `nn.L1Loss()` and `torch.optim.SGD(params, lr)` respectively.
  * Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.
  * Write a training loop to perform the appropriate training steps for 300 epochs.
  * The training loop should test the model on the test dataset every 20 epochs.


```python
# Create the loss function and optimizer
loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(params=model_1.parameters(),
                            lr = 0.01)
```


```python
# Training loop
torch.manual_seed(42)

# Train model for 300 epochs
epochs = 300

# Send data to target device
X_train = X_train.to(device)
y_train = y_train.to(device)
X_test = X_test.to(device)
y_test = y_test.to(device)
```

--> 여기서 헤맸다. 데이터를 cuda에 올릴 때는 따로 변수를 지정해줘야한다.

```python
for epoch in range(epochs):
  ### Training

  # Put model in train mode
  model_1.train()


  # 1. Forward pass
  y_pred = model_1(X_train)

  # 2. Calculate loss
  loss = loss_fn(y_pred, y_train)

  # 3. Zero gradients
  optimizer.zero_grad()

  # 4. Backpropagation
  loss.backward()

  # 5. Step the optimizer
  optimizer.step()

  ### Perform testing every 20 epochs
  if epoch % 20 == 0:

    # Put model in evaluation mode and setup inference context
    model_1.eval()
    with torch.inference_mode():

      # 1. Forward pass
      test_pred = model_1(X_test)
      # 2. Calculate test loss
      test_loss = loss_fn(test_pred, y_test)
      # Print out what's happening
      print(f"Epoch: {epoch} | Train loss: {loss:.3f} | Test loss: {test_loss:.3f}")
```

    Epoch: 0 | Train loss: 0.128 | Test loss: 0.337
    Epoch: 20 | Train loss: 0.082 | Test loss: 0.218
    Epoch: 40 | Train loss: 0.072 | Test loss: 0.175
    Epoch: 60 | Train loss: 0.065 | Test loss: 0.153
    Epoch: 80 | Train loss: 0.058 | Test loss: 0.137
    Epoch: 100 | Train loss: 0.051 | Test loss: 0.121
    Epoch: 120 | Train loss: 0.045 | Test loss: 0.104
    Epoch: 140 | Train loss: 0.038 | Test loss: 0.088
    Epoch: 160 | Train loss: 0.031 | Test loss: 0.072
    Epoch: 180 | Train loss: 0.024 | Test loss: 0.056
    Epoch: 200 | Train loss: 0.017 | Test loss: 0.040
    Epoch: 220 | Train loss: 0.010 | Test loss: 0.024
    Epoch: 240 | Train loss: 0.003 | Test loss: 0.007
    Epoch: 260 | Train loss: 0.008 | Test loss: 0.007
    Epoch: 280 | Train loss: 0.008 | Test loss: 0.007


## 4. Make predictions with the trained model on the test data.
  * Visualize these predictions against the original training and testing data (**note:** you may need to make sure the predictions are *not* on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).


```python
# Make predictions with the model
model_1.eval()
with torch.inference_mode():
  y_preds = model_1(X_test)
y_preds.cpu()
```




    tensor([[1.1333],
            [1.1363],
            [1.1393],
            [1.1423],
            [1.1454],
            [1.1484],
            [1.1514],
            [1.1545],
            [1.1575],
            [1.1605],
            [1.1635],
            [1.1666],
            [1.1696],
            [1.1726],
            [1.1757],
            [1.1787],
            [1.1817],
            [1.1847],
            [1.1878],
            [1.1908]])




```python
# Plot the predictions (these may need to be on a specific device)
plot_predictions(predictions=y_preds.cpu())
```


    
![01_pytorch_workflow_exercises_18_0](https://github.com/yesnote/yesnote.github.io/assets/173476188/311fe3a2-d503-4278-b216-ff28f7e57910)



## 5. Save your trained model's `state_dict()` to file.
  * Create a new instance of your model class you made in 2. and load in the `state_dict()` you just saved to it.
  * Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.


```python
from pathlib import Path

# 1. Create models directory
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)

# 2. Create model save path
MODEL_NAME = "saved_model.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# 3. Save the model state dict
torch.save(obj=model_1.state_dict(),
           f=MODEL_SAVE_PATH)

```


```python
# Create new instance of model and load saved state dict (make sure to put it on the target device)
loaded_model_1 = LinearRegressionModel()
loaded_model_1.load_state_dict(torch.load(f=MODEL_SAVE_PATH))
loaded_model_1.to(device)
loaded_model_1.state_dict()
```




    OrderedDict([('linear_layer.weight', tensor([[0.3028]], device='cuda:0')),
                 ('linear_layer.bias', tensor([0.8910], device='cuda:0'))])




```python
# Make predictions with loaded model and compare them to the previous
loaded_y_preds = loaded_model_1(X_test)
loaded_y_preds == y_preds
```




    tensor([[True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True],
            [True]], device='cuda:0')




```python

```


# 참고자료

[Ground truth notebook](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb){: target="_blank"}

[Book version of notebook](https://www.learnpytorch.io/01_pytorch_workflow/){: target="_blank"}

[Ask a question](https://github.com/mrdbourke/pytorch-deep-learning/discussions){: target="_blank"}