---
layout: single
title:  "[PyTorch for Deep Learning] 03 PyTorch Computer Vision - 3. Model 0: Build a baseline model"
categories: [Machine Learning]
tag: [pytorch, coding, [computer vision]]
use_math: true
---

이 글은 Computer vision with PyTorch 공부에 관한 기록입니다.



# Model 0: Build a basline model

When starting to build a series of machine learning modelling experiments, it's best practice to start with a baseline model.

A baseline model is a simple model you will try and improve upon with subsequent models/experiments.

In other words: start simply and add complexity when necessary.


```python
# Create a flatten layer
flatten_model = nn.Flatten() 

# Get a single sample
x = train_features_batch[0]

# Flatten the sample
output = flatten_model(x) # perform forward pass

# Print out what happened
print(f"Shape before flattening: {x.shape} -> [color_channels, height, width]")
print(f"Shape after flattening: {output.shape} -> [color_channels, height*width]")
```

    Shape before flattening: torch.Size([1, 28, 28]) -> [color_channels, height, width]
    Shape after flattening: torch.Size([1, 784]) -> [color_channels, height*width]

> `nn.Flatten()`: 텐서를 **벡터화**한다.

```python
from torch import nn 
class FashionMNISTModelV0(nn.Module):
  def __init__(self,
               input_shape: int,
               hidden_units: int,
               output_shape: int):
    super().__init__()
    self.layer_stack = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features=input_shape,
                  out_features=hidden_units),
        nn.Linear(in_features=hidden_units,
                  out_features=output_shape)
    )

  def forward(self, x):
    return self.layer_stack(x)
```


```python
torch.manual_seed(42)

# Setup model with input parameters
model_0 = FashionMNISTModelV0(
    input_shape=28*28, # this is 28*28
    hidden_units=10, # how mnay units in the hidden layer
    output_shape=len(class_names) # one for every class
).to("cpu")

model_0
```




    FashionMNISTModelV0(
      (layer_stack): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=10, bias=True)
        (2): Linear(in_features=10, out_features=10, bias=True)
      )
    )

> `nn.Flatten()`에 의해 벡터화된 후에 `nn.Linear()`를 거치므로 `input_shape=28*28`.

> *Classification* 모델이므로 `output_shape`은 클래스 종류의 수와 같아야 한다.


```python
dummy_x = torch.rand([1, 1, 28, 28])
model_0(dummy_x)
```




    tensor([[-0.0315,  0.3171,  0.0531, -0.2525,  0.5959,  0.2112,  0.3233,  0.2694,
             -0.1004,  0.0157]], grad_fn=<AddmmBackward0>)




```python
model_0.state_dict()
```




    OrderedDict([('layer_stack.1.weight',
                  tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],
                          [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],
                          [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],
                          ...,
                          [-0.0116,  0.0273, -0.0344,  ...,  0.0176,  0.0283, -0.0011],
                          [-0.0230,  0.0257,  0.0291,  ..., -0.0187, -0.0087,  0.0001],
                          [ 0.0176, -0.0147,  0.0053,  ..., -0.0336, -0.0221,  0.0205]])),
                 ('layer_stack.1.bias',
                  tensor([-0.0093,  0.0283, -0.0033,  0.0255,  0.0017,  0.0037, -0.0302, -0.0123,
                           0.0018,  0.0163])),
                 ('layer_stack.2.weight',
                  tensor([[ 0.0614, -0.0687,  0.0021,  0.2718,  0.2109,  0.1079, -0.2279, -0.1063,
                            0.2019,  0.2847],
                          [-0.1495,  0.1344, -0.0740,  0.2006, -0.0475, -0.2514, -0.3130, -0.0118,
                            0.0932, -0.1864],
                          [ 0.2488,  0.1500,  0.1907,  0.1457, -0.3050, -0.0580,  0.1643,  0.1565,
                           -0.2877, -0.1792],
                          [ 0.2305, -0.2618,  0.2397, -0.0610,  0.0232,  0.1542,  0.0851, -0.2027,
                            0.1030, -0.2715],
                          [-0.1596, -0.0555, -0.0633,  0.2302, -0.1726,  0.2654,  0.1473,  0.1029,
                            0.2252, -0.2160],
                          [-0.2725,  0.0118,  0.1559,  0.1596,  0.0132,  0.3024,  0.1124,  0.1366,
                           -0.1533,  0.0965],
                          [-0.1184, -0.2555, -0.2057, -0.1909, -0.0477, -0.1324,  0.2905,  0.1307,
                           -0.2629,  0.0133],
                          [ 0.2727, -0.0127,  0.0513,  0.0863, -0.1043, -0.2047, -0.1185, -0.0825,
                            0.2488, -0.2571],
                          [ 0.0425, -0.1209, -0.0336, -0.0281, -0.1227,  0.0730,  0.0747, -0.1816,
                            0.1943,  0.2853],
                          [-0.1310,  0.0645, -0.1171,  0.2168, -0.0245, -0.2820,  0.0736,  0.2621,
                            0.0012, -0.0810]])),
                 ('layer_stack.2.bias',
                  tensor([-0.0087,  0.1791,  0.2712, -0.0791,  0.1685,  0.1762,  0.2825,  0.2266,
                          -0.2612, -0.2613]))])



## Setup loss, optimizer and evaluation metrics

* Loss function - since we're working with multi-class data, our loss function will be `nn.CrossEntropyLoss()`
* Optimizer - our optimizer `torch.optim.SGD()` (stochastic gradient descent)
* Evaluation metric - since we're working on a classification problem, let's use accruacy as our evaluation metric


```python
import requests
from pathlib import Path

# Download helper functions from Learn PyTorch repo
if Path("helper_functions.py").is_file():
  print("helper_functions.py already exists, skipping download...")
else:
  print("Downloading helper_functions.py")
  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py")
  with open("helper_functions.py", "wb") as f:
    f.write(request.content)
```

    helper_functions.py already exists, skipping download...



```python
# Import accuracy metric
from helper_functions import accuracy_fn

# Setup loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr=0.1)
```

## Creating a function to time our experiments

Machine learning is very experimental.

Two of the main things you'll often want to track are:
1. Model's performance (loss and accuracy values etc)
2. How fast it runs 


```python
from timeit import default_timer as timer 
def print_train_time(start: float,
                     end: float, 
                     device: torch.device = None):
  """Prints difference between start and end time."""
  total_time = end - start
  print(f"Train time on {device}: {total_time:.3f} seconds")
  return total_time
```


```python
start_time = timer()
# some code...
end_time = timer()
print_train_time(start=start_time, end=end_time, device="cpu")
```

    Train time on cpu: 0.000 seconds





    2.397800017206464e-05



## Creating a training loop and training a model on batches of data

1. Loop through epochs.
2. Loop through training batches, perform training steps, calculate the train loss *per batch*.
3. Loop through testing batches, perform testing steps, calculate the test loss *per batch*.
4. Print out what's happening.
5. Time it all (for fun).

**Note:** Because we are computing on *batches*, the optimizer will update the model's parameters once *per batch* rather than once per epoch.

> 에폭에 따라서가 아니라, 각 배치마다 Loss를 계산하고 Step하는 것을 이해해야한다. 또한 각 배치별로 한번에 forward passing한다.

```python
# Import tqdm for progress bar
from tqdm.auto import tqdm

# Set the seed and start the timer
torch.manual_seed(42)
train_time_start_on_cpu = timer() 

# Set the number of epochs (we'll keep this small for faster training time)
epochs = 3

# Create training and test loop
for epoch in tqdm(range(epochs)):
  print(f"Epoch: {epoch}\n------")
  ### Training
  train_loss = 0
  # Add a loop to loop through the training batches
  for batch, (X, y) in enumerate(train_dataloader):
    model_0.train()
    # 1. Forward pass
    y_pred = model_0(X)
    
    # 2. Calculate loss (per batch)
    loss = loss_fn(y_pred, y)
    train_loss += loss # accumulate train loss
    
    # 3. Optimizer zero grad
    optimizer.zero_grad()
    
    # 4. Loss backward
    loss.backward()
    
    # 5. Optimizer step (update the model's parameters once *per batch*)
    optimizer.step()
    
    # Print out what's happening
    if batch % 400 == 0:
        print(f"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples.")
  
  # Divide total train loss by length of train dataloader
  train_loss /= len(train_dataloader)

  ### Testing
  test_loss, test_acc = 0, 0
  model_0.eval()
  with torch.inference_mode(): 
    for X_test, y_test in test_dataloader:
      # 1. Forward pass
      test_pred = model_0(X_test)

      # 2. Calculate loss (accumulatively)
      test_loss += loss_fn(test_pred, y_test)

      # 3. Calculate accuracy
      test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))

    # Calculate the test loss average per batch
    test_loss /= len(test_dataloader)

    # Calculate the test acc average per batch
    test_acc /= len(test_dataloader)

  # Print out what's happening
  print(f"\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}")

# Calculate training time
train_time_end_on_cpu = timer()
total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,
                                            end=train_time_end_on_cpu,
                                            device=str(next(model_0.parameters()).device))
```


      0%|          | 0/3 [00:00<?, ?it/s]


    Epoch: 0
    ------
    Looked at 0/60000 samples.
    Looked at 12800/60000 samples.
    Looked at 25600/60000 samples.
    Looked at 38400/60000 samples.
    Looked at 51200/60000 samples.
    
    Train loss: 0.5904 | Test loss: 0.5095, Test acc: 82.0387
    Epoch: 1
    ------
    Looked at 0/60000 samples.
    Looked at 12800/60000 samples.
    Looked at 25600/60000 samples.
    Looked at 38400/60000 samples.
    Looked at 51200/60000 samples.
    
    Train loss: 0.4763 | Test loss: 0.4799, Test acc: 83.1969
    Epoch: 2
    ------
    Looked at 0/60000 samples.
    Looked at 12800/60000 samples.
    Looked at 25600/60000 samples.
    Looked at 38400/60000 samples.
    Looked at 51200/60000 samples.
    
    Train loss: 0.4550 | Test loss: 0.4766, Test acc: 83.4265
    Train time on cpu: 26.749 seconds

# 참고자료

* [Reference notebook](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb){: target="_blank"}
* [Reference online book](https://www.learnpytorch.io/03_pytorch_computer_vision/){: target="_blank"}