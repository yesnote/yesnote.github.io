---
layout: single
title:  "[PyTorch for Deep Learning] 02 Neural Network classification - 2. Build Model"
categories: [Machine Learning]
tag: [pytorch, coding]
use_math: true
---

이 글은 PyTorch를 이용한 Neural network classification 공부에 관한 기록입니다.


# Build Model

Let's build a model to classify our blue and red dots.

To do so, we want to:
1. Setup device agonistic code so our code will run on an accelerator (GPU) if there is one
2. Construct a model (by subclassing `nn.Module`)
3. Define a loss function and optimizer
4. Create a training and test loop


```python
# Import PyTorch and nn
import torch
from torch import nn

# Make device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device
```




    'cuda'




```python
X_train
```




    tensor([[ 0.6579, -0.4651],
            [ 0.6319, -0.7347],
            [-1.0086, -0.1240],
            ...,
            [ 0.0157, -1.0300],
            [ 1.0110,  0.1680],
            [ 0.5578, -0.5709]])



Now we've setup device agnostic code, let's create a model that:

1. Subclasses `nn.Module` (almost all models in PyTorch subclass `nn.Module`)
2. Create 2 `nn.Linear()` layers that are capable of handling the shapes of our data
3. Defines a `forward()` method that outlines the forward pass (or forward computation) of the model
4. Instatiate an instance of our model class and send it to the target `device`

--> 비선형 모델밖에 없으므로 원형의 데이터를 잘 구분하지 못할 것을 예측할 수 있다.

```python
X_train.shape
```




    torch.Size([800, 2])




```python
y_train[:5]
```




    tensor([1., 0., 0., 0., 1.])




```python
from sklearn import datasets
# 1. Construct a model that subclasses nn.Module
class CircleModelV0(nn.Module):
  def __init__(self):
    super().__init__()
    # 2. Create 2 nn.Linear layers capable of handling the shapes of our data
    self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features and upscales to 5 features 
    self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous layer and outputs a single feature (same shape as y)

  # 3. Define a forward() method that outlines the forward pass
  def forward(self, x):
    return self.layer_2(self.layer_1(x)) # x -> layer_1 ->  layer_2 -> output

# 4. Instantiate an instance of our model class and send it to the target device
model_0 = CircleModelV0().to(device)
model_0
```




    CircleModelV0(
      (layer_1): Linear(in_features=2, out_features=5, bias=True)
      (layer_2): Linear(in_features=5, out_features=1, bias=True)
    )

--> X가 x좌표, y좌표 2가지 feature를 가지므로 in_features=2

--> Binary classification이므로 out_features=1


```python
device
```




    'cuda'




```python
next(model_0.parameters()).device
```




    device(type='cuda', index=0)




```python
# Let's replicate the model above using nn.Sequential()
model_0 = nn.Sequential(
    nn.Linear(in_features=2, out_features=5),
    nn.Linear(in_features=5, out_features=1)
).to(device)

model_0
```




    Sequential(
      (0): Linear(in_features=2, out_features=5, bias=True)
      (1): Linear(in_features=5, out_features=1, bias=True)
    )




```python
model_0.state_dict()
```




    OrderedDict([('0.weight', tensor([[-0.1962,  0.3652],
                          [ 0.2764,  0.2147],
                          [ 0.5723, -0.5955],
                          [-0.2329,  0.0170],
                          [ 0.6259,  0.6916]], device='cuda:0')),
                 ('0.bias',
                  tensor([ 0.4193,  0.6624,  0.2594, -0.1640, -0.0477], device='cuda:0')),
                 ('1.weight',
                  tensor([[ 0.4129,  0.2358,  0.4115,  0.4045, -0.0853]], device='cuda:0')),
                 ('1.bias', tensor([-0.4294], device='cuda:0'))])


--> 뉴런이 **2개 -> 5개 -> 1개**이므로 2x5=10, 5x1=5개만큼의 weights를 생성함.


```python
# Make predictions
with torch.inference_mode():
  untrained_preds = model_0(X_test.to(device))
print(f"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}")
print(f"Length of test samples: {len(X_test)}, Shape: {X_test.shape}")
print(f"\nFirst 10 predictions:\n{torch.round(untrained_preds[:10])}")
print(f"\nFirst 10 labels:\n{y_test[:10]}")
```

    Length of predictions: 200, Shape: torch.Size([200, 1])
    Length of test samples: 200, Shape: torch.Size([200, 2])
    
    First 10 predictions:
    tensor([[-0.],
            [-0.],
            [-0.],
            [-0.],
            [0.],
            [0.],
            [-0.],
            [-0.],
            [-0.],
            [-0.]], device='cuda:0')
    
    First 10 labels:
    tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])



```python
X_test[:10], y_test[:10]
```




    (tensor([[-0.3752,  0.6827],
             [ 0.0154,  0.9600],
             [-0.7028, -0.3147],
             [-0.2853,  0.9664],
             [ 0.4024, -0.7438],
             [ 0.6323, -0.5711],
             [ 0.8561,  0.5499],
             [ 1.0034,  0.1903],
             [-0.7489, -0.2951],
             [ 0.0538,  0.9739]]),
     tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.]))



## Setup loss function and optimizer

Which loss function or optimizer should you use?

Again... this is problem specific.

For example for regression you might want MAE or MSE (mean absolute error or mean squared error).

For classification you might want binary cross entropy or categorical cross entropy (cross entropy).

As a reminder, the loss function measures how *wrong* your models predictions are.

And for optimizers, two of the most common and useful are SGD and Adam, however PyTorch has many built-in options.

* For some common choices of loss functions and optimizers - [loss functions and optimizers](https://www.learnpytorch.io/02_pytorch_classification/#21-setup-loss-function-and-optimizer){: target="_blank"}
* For the loss function we're going to use `torch.nn.BECWithLogitsLoss()`, for more on what binary cross entropy (BCE) is, check out this article - [BCE](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a){: target="_blank"}
* For a defintion on what a logit is in deep learning - [Logits](https://stackoverflow.com/a/52111173/7900723){: target="_blank"}
* For different optimizers see [`torch.optim`](https://stackoverflow.com/a/52111173/7900723){: target="_blank"}


```python
# Setup the loss function
# loss_fn = nn.BCELoss() # BCELoss = requires inputs to have gone through the sigmoid activation function prior to input to BCELoss
loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid activation function built-in

optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr=0.1)
```


```python
# Calculate accuracy - out of 100 examples, what percentage does our model get right? 
def accuracy_fn(y_true, y_pred):
  correct = torch.eq(y_true, y_pred).sum().item() 
  acc = (correct/len(y_pred)) * 100
  return acc
```

# 참고자료

[Ground truth notebook](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/02_pytorch_classification.ipynb){: target="_blank"}

[Book version of notebook](https://www.learnpytorch.io/02_pytorch_classification/){: target="_blank"}

[Ask a question](https://github.com/mrdbourke/pytorch-deep-learning/discussions){: target="_blank"}